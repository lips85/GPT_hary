{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment4 RAG (GPT MINI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.ollama import ChatOllama\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.document_loaders.unstructured import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "LLM_model, models = [\"openai\", \"gpt-4o-mini-2024-07-18\"]\n",
    "# LLM_model, models = [\"ollama\", \"gemma2:latest\"]\n",
    "\n",
    "file_name = \"document.txt\"\n",
    "\n",
    "llm = (\n",
    "    ChatOllama(temperature=0.1, model=models)\n",
    "    if LLM_model == \"ollama\"\n",
    "    else ChatOpenAI(temperature=0.1, model=models)\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(f\"../files/{file_name}\")\n",
    "cache_dir = LocalFileStore(f\"../.cache/embeddings/{LLM_model}/{models}/{file_name}\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separators=[\"\\n\\n\", \".\", \"?\", \"!\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = (\n",
    "    OllamaEmbeddings(model=models) if LLM_model == \"ollama\" else OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    "    memory_key=\"history\",\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            당신은 문서를 읽고 대답해주는 AI입니다. 주어지는 문서를 통해 답변을 주세요. \n",
    "            만약 정보가 문서에 없다면, '모르겠습니다'라고 답변해주세요.\n",
    "            한국어로 답변해주세요.\n",
    "            \n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": RunnableLambda(load_memory),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question).content\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서에 따르면, Winston은 Aaronson이 범죄로 기소된 것에 대해 유죄라고 믿고 있으며, 그가 범죄를 저지른 증거가 없다는 사실을 기억하지 못한다고 언급하고 있습니다. 따라서 문서의 내용에 따르면 Aaronson은 유죄로 간주되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Is Aaronson guilty?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winston은 테이블에 \"FREEDOM IS SLAVERY\"와 그 아래에 \"TWO AND TWO MAKE FIVE\"라는 메시지를 썼습니다.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What message did he write in the table?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia는 Winston의 사랑하는 사람으로, 그들의 관계는 문서에서 중요한 주제 중 하나입니다. 그녀는 Winston과 함께 자유롭고 반항적인 감정을 나누었지만, 후에 그들의 관계는 고통과 배신으로 이어지게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Who is Julia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신이 처음으로 한 질문은 \"Aaronson guilty?\"였습니다.\n",
      "당신이 두 번째로 한 질문은 \"What message did he write in the table?\"였습니다.\n",
      "당신이 세 번째로 한 질문은 \"Who is Julia?\"였습니다.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What was the first question I asked?\")\n",
    "\n",
    "invoke_chain(\"What was the second question I asked?\")\n",
    "\n",
    "invoke_chain(\"What was the third question I asked?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Is Aaronson guilty?'), AIMessage(content='문서에 따르면, Winston은 Aaronson이 범죄로 기소된 것에 대해 유죄라고 믿고 있으며, 그가 범죄를 저지른 증거가 없다는 사실을 기억하지 못한다고 언급하고 있습니다. 따라서 문서의 내용에 따르면 Aaronson은 유죄로 간주되고 있습니다.'), HumanMessage(content='What message did he write in the table?'), AIMessage(content='Winston은 테이블에 \"FREEDOM IS SLAVERY\"와 그 아래에 \"TWO AND TWO MAKE FIVE\"라는 메시지를 썼습니다.'), HumanMessage(content='Who is Julia?'), AIMessage(content='Julia는 Winston의 사랑하는 사람으로, 그들의 관계는 문서에서 중요한 주제 중 하나입니다. 그녀는 Winston과 함께 자유롭고 반항적인 감정을 나누었지만, 후에 그들의 관계는 고통과 배신으로 이어지게 됩니다.'), HumanMessage(content='What was the first question I asked?'), AIMessage(content='당신이 처음으로 한 질문은 \"Aaronson guilty?\"였습니다.'), HumanMessage(content='What was the second question I asked?'), AIMessage(content='당신이 두 번째로 한 질문은 \"What message did he write in the table?\"였습니다.'), HumanMessage(content='What was the third question I asked?'), AIMessage(content='당신이 세 번째로 한 질문은 \"Who is Julia?\"였습니다.')]), return_messages=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
