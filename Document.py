import re
import os
import streamlit as st

from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders.unstructured import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.vectorstores.faiss import FAISS
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.callbacks.base import BaseCallbackHandler

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
    layout="wide",
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
for key, default in [
    ("messages", []),
    ("api_key", None),
    ("api_key_check", False),
    ("openai_model", "ì„ íƒí•´ì£¼ì„¸ìš”"),
]:
    if key not in st.session_state:
        st.session_state[key] = default

# ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´
API_KEY_pattern = r"sk-.*"
Model_pattern = r"gpt-*"

# OpenAI ëª¨ë¸ ëª©ë¡
openai_models = ["ì„ íƒí•´ì£¼ì„¸ìš”", "gpt-4o-mini-2024-07-18", "gpt-4o-mini-2024-07-18"]

# í˜ì´ì§€ ì œëª© ë° ì„¤ëª…
st.title("DocumentGPT")
st.markdown(
    """
    ì•ˆë…•í•˜ì„¸ìš”! ì´ í˜ì´ì§€ëŠ” ë¬¸ì„œë¥¼ ì½ì–´ì£¼ëŠ” AIì…ë‹ˆë‹¤.ğŸ˜„ 
    
    ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ í•˜ë©´ ë¬¸ì„œì— ëŒ€í•œ ë‹µë³€ì„ í•´ì¤ë‹ˆë‹¤.
    """
)


# ì½œë°± í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤
class ChatCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        self.message = ""
        self.message_box = None

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


# íŒŒì¼ ì„ë² ë”© í•¨ìˆ˜
@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file):
    os.makedirs("./.cache/files", exist_ok=True)
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file.read())

    cache_dir = LocalFileStore(f"./.cache/embeddings/open_ai/{file.name}")
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        separators=["\n\n", ".", "?", "!"],
        chunk_size=1000,
        chunk_overlap=100,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings(openai_api_key=st.session_state["api_key"])
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    return vectorstore.as_retriever()


# ë©”ì‹œì§€ ì €ì¥ í•¨ìˆ˜
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})


# ë©”ì‹œì§€ ì „ì†¡ í•¨ìˆ˜
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)


# ì±„íŒ… ê¸°ë¡ í‘œì‹œ í•¨ìˆ˜
def paint_history():
    for message in st.session_state["messages"]:
        send_message(message["message"], message["role"], save=False)


# ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜
def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)


# API í‚¤ ì €ì¥ í•¨ìˆ˜
def save_api_key(api_key):
    st.session_state["api_key"] = api_key
    st.session_state["api_key_check"] = True


# OpenAI ëª¨ë¸ ì €ì¥ í•¨ìˆ˜
def save_openai_model(openai_model):
    st.session_state["openai_model"] = openai_model
    st.session_state["openai_model_check"] = True


# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file", type=["pdf", "txt", "docx"]
    )
    api_key = st.text_input("API_KEY ì…ë ¥", placeholder="sk-...").strip()

    if api_key:
        save_api_key(api_key)
        st.write("ğŸ˜„API_KEYê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.ğŸ˜„")

    if st.button("ì €ì¥"):
        save_api_key(api_key)
        if not api_key:
            st.warning("OPENAI_API_KEYë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")

    openai_model = st.selectbox("OpneAI Modelì„ ê³¨ë¼ì£¼ì„¸ìš”.", options=openai_models)
    if openai_model != "ì„ íƒí•´ì£¼ì„¸ìš”" and re.match(Model_pattern, openai_model):
        save_openai_model(openai_model)
        st.write("ğŸ˜„ëª¨ë¸ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.ğŸ˜„")

    st.write(
        """
        Made by hary.
        
        Github
        https://github.com/lips85/GPT_hary

        streamlit
        https://hary-gpt.streamlit.app/
        """
    )

# ë©”ì¸ ë¡œì§
if st.session_state["api_key_check"] and st.session_state["api_key"]:
    llm = ChatOpenAI(
        temperature=0.1,
        streaming=True,
        callbacks=[ChatCallbackHandler()],
        model=st.session_state["openai_model"],
        openai_api_key=st.session_state["api_key"],
    )

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                You are an AI that reads documents for me. Please answer based on the document given below. 
                If the information is not in the document, answer the question with "The required information is not in the document." Never make up answers.
                Please answer in the questioner's language 
                
                Context : {context}
                """,
            ),
            ("human", "{question}"),
        ]
    )

    if file:
        retriever = embed_file(file)
        send_message("I'm ready! Ask away!", "ai", save=False)
        paint_history()
        message = st.chat_input("Ask anything about your file...")

        if message:
            if re.match(API_KEY_pattern, st.session_state["api_key"]) and re.match(
                Model_pattern, st.session_state["openai_model"]
            ):
                send_message(message, "human")
                chain = (
                    {
                        "context": retriever | RunnableLambda(format_docs),
                        "question": RunnablePassthrough(),
                    }
                    | prompt
                    | llm
                )
                try:
                    with st.chat_message("ai"):
                        chain.invoke(message)
                except Exception as e:
                    st.error(f"An error occurred: {e}")
                    st.warning("OPENAI_API_KEY or ëª¨ë¸ ì„ íƒì„ ë‹¤ì‹œ ì§„í–‰í•´ì£¼ì„¸ìš”.")
            else:
                send_message(
                    "OPENAI_API_KEY or ëª¨ë¸ ì„ íƒì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.",
                    "ai",
                )
    else:
        st.session_state["messages"] = []
